{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fec6cf",
   "metadata": {},
   "source": [
    "Dataset : https://drive.google.com/drive/folders/1ZDVv9fsCaD3FdCp8RaoKKGFPfvmWbayd?usp=drive_link\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191bfa43",
   "metadata": {},
   "source": [
    "This code implements a few-shot learning model for wheat disease classification using EfficientNet for feature extraction and an attention module for similarity learning, trained with cross-entropy loss and evaluated for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8dba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37f6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Class\n",
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2f686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load Dataset and Split into Train/Test\n",
    "def load_data(data_dir, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Load dataset and split into train and test.\n",
    "    :param data_dir: Path to dataset where folders represent classes.\n",
    "    :param train_split: Ratio of training data (default: 80% train, 20% test).\n",
    "    :return: Train and test datasets.\n",
    "    \"\"\"\n",
    "    classes = sorted(os.listdir(data_dir))  # Get class names\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(data_dir, cls)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue  # Skip non-directory files\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_to_idx[cls])\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    dataset = list(zip(image_paths, labels))\n",
    "    random.shuffle(dataset)\n",
    "    image_paths, labels = zip(*dataset)\n",
    "\n",
    "    # Split into train and test\n",
    "    train_size = int(train_split * len(image_paths))\n",
    "    train_images, test_images = image_paths[:train_size], image_paths[train_size:]\n",
    "    train_labels, test_labels = labels[:train_size], labels[train_size:]\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels, class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fe0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset_path = \"./D3_Final\"  # Update this with your dataset path\n",
    "train_images, train_labels, test_images, test_labels, class_to_idx = load_data(dataset_path)\n",
    "\n",
    "train_dataset = WheatDataset(train_images, train_labels, transform=transform)\n",
    "test_dataset = WheatDataset(test_images, test_labels, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866b7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extractor (EfficientNet)\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.efficient_net = models.efficientnet_b0(pretrained=True)\n",
    "        self.efficient_net.classifier = nn.Identity()  # Remove last layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficient_net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfcc58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.W1 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.W2 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.omega_x = nn.Parameter(torch.rand(feature_dim))\n",
    "        self.omega_y = nn.Parameter(torch.rand(feature_dim))\n",
    "        self.omega_xy = nn.Parameter(torch.rand(feature_dim))\n",
    "\n",
    "    def forward(self, support, query):\n",
    "        s = torch.relu(self.W1(support))\n",
    "        q = torch.relu(self.W2(query))\n",
    "        similarity = torch.sum(self.omega_x * s + self.omega_y * q + self.omega_xy * (s * q), dim=-1)\n",
    "\n",
    "        # Ensure output matches EfficientNet feature size (1280)\n",
    "        output = torch.relu(self.W1(support) + self.W2(query))\n",
    "        return output  # Output must be (batch_size, 1280)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df332f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot Learning Network\n",
    "class FewShotNetwork(nn.Module):\n",
    "    def __init__(self, feature_dim=1280, num_classes=18):\n",
    "        super(FewShotNetwork, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.attention = AttentionModule(feature_dim)\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, support, query):\n",
    "        support_features = self.feature_extractor(support)\n",
    "        query_features = self.feature_extractor(query)\n",
    "        similarity = self.attention(support_features, query_features)\n",
    "        output = self.fc(similarity)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be03eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FewShotNetwork().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34d3d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4844\n",
      "Epoch [2/20], Loss: 0.2233\n",
      "Epoch [3/20], Loss: 0.2047\n",
      "Epoch [4/20], Loss: 0.1397\n",
      "Epoch [5/20], Loss: 0.1512\n",
      "Epoch [6/20], Loss: 0.1362\n",
      "Epoch [7/20], Loss: 0.1829\n",
      "Epoch [8/20], Loss: 0.0775\n",
      "Epoch [9/20], Loss: 0.0683\n",
      "Epoch [10/20], Loss: 0.0775\n",
      "Epoch [11/20], Loss: 0.0542\n",
      "Epoch [12/20], Loss: 0.0539\n",
      "Epoch [13/20], Loss: 0.1432\n",
      "Epoch [14/20], Loss: 0.0579\n",
      "Epoch [15/20], Loss: 0.0291\n",
      "Epoch [16/20], Loss: 0.0388\n",
      "Epoch [17/20], Loss: 0.0397\n",
      "Epoch [18/20], Loss: 0.0276\n",
      "Epoch [19/20], Loss: 0.0629\n",
      "Epoch [20/20], Loss: 0.0595\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "def train_model(model, train_loader, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, images)  # Support and Query Set (same for now)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a3f275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.61%\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images, images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6e0d2",
   "metadata": {},
   "source": [
    "--------------------- Add more visualisation --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbcd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images, images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calcul de la précision\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels)) * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Génération de la Matrice de Confusion\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Affichage de la Matrice de Confusion\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Prédictions\")\n",
    "    plt.ylabel(\"Vraies Classes\")\n",
    "    plt.title(\"Matrice de Confusion\")\n",
    "    plt.show()\n",
    "\n",
    "    # Rapport de Classification\n",
    "    print(\"\\nRapport de Classification :\\n\", classification_report(all_labels, all_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curve(train_losses, test_losses, test_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Courbe de la Perte\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(epochs, test_losses, label=\"Test Loss\", marker='o')\n",
    "    plt.xlabel(\"Épochs\")\n",
    "    plt.ylabel(\"Perte\")\n",
    "    plt.title(\"Évolution de la Perte\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Courbe de la Précision\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, test_accuracies, label=\"Test Accuracy\", marker='o', color='green')\n",
    "    plt.xlabel(\"Épochs\")\n",
    "    plt.ylabel(\"Précision (%)\")\n",
    "    plt.title(\"Évolution de la Précision\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Brown rust\", \"Yellow rust\", \"Stem rust\"]\n",
    "evaluate_model(model, test_loader, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
